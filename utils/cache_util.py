"""
ç¼“å­˜å·¥å…·æ¨¡å—

æä¾›å®Œæ•´çš„ç¼“å­˜ç®¡ç†åŠŸèƒ½ï¼Œæ”¯æŒä»¥ä¸‹ç‰¹æ€§ï¼š

åŠŸèƒ½ç‰¹æ€§:
    - å¤šç§ç¼“å­˜å®ç°
        * Redis ç¼“å­˜
        * LRU å†…å­˜ç¼“å­˜
        * ç£ç›˜ç¼“å­˜
        * JSONæ–‡ä»¶ç¼“å­˜
    - ç¼“å­˜è£…é¥°å™¨
        * å‡½æ•°ç»“æœç¼“å­˜
        * ç±»æ–¹æ³•ç»“æœç¼“å­˜
        * ä¾èµ–å‡½æ•°ç¼“å­˜
    - ç¼“å­˜ç­–ç•¥
        * è¿‡æœŸæ—¶é—´æ§åˆ¶
        * æœ€å¤§å®¹é‡æ§åˆ¶
        * è‡ªåŠ¨æ¸…ç†æœºåˆ¶
    - åºåˆ—åŒ–æ”¯æŒ
        * JSON åºåˆ—åŒ–
        * Pickle åºåˆ—åŒ–
    - ç¼“å­˜ç»Ÿè®¡
        * å‘½ä¸­ç‡ç»Ÿè®¡
        * ä½¿ç”¨é‡ç»Ÿè®¡

æŠ€æœ¯ç‰¹ç‚¹:
    - è£…é¥°å™¨æ¨¡å¼
    - ç­–ç•¥æ¨¡å¼
    - å·¥å‚æ¨¡å¼
    - å®Œæ•´çš„ç±»å‹æ³¨è§£
    - è‡ªåŠ¨åŒ–æ—¥å¿—è®°å½•
"""

import functools
import json
import os
import pickle
import shutil
import tempfile
import time
import uuid
from abc import ABC, abstractmethod
from collections import OrderedDict
from dataclasses import dataclass
from threading import Lock
from typing import (Any, Callable, Dict, Optional, Text,
                    TypeVar, Union, cast, Protocol)

import redis

from .log_util import my_logger

# ç±»å‹å˜é‡å®šä¹‰
T = TypeVar('T')  # ç”¨äºæ³›å‹æ–¹æ³•è¿”å›å€¼
CacheKey = Union[str, int]  # ç¼“å­˜é”®ç±»å‹
CacheValue = Any  # ç¼“å­˜å€¼ç±»å‹

# ç£ç›˜ç¼“å­˜çš„é»˜è®¤è·¯å¾„
DISK_CACHE_PATH = os.path.join(tempfile.gettempdir(), ".diskcache")
JSON_CACHE_PATH = os.path.join(tempfile.gettempdir(), "cache_data.json")

T_co = TypeVar("T_co", covariant=True)


class SupportsWrite(Protocol[T_co]):
    def write(self, __s: T_co) -> Any: ...


@dataclass
class CacheStats:
    """ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
    hits: int = 0  # å‘½ä¸­æ¬¡æ•°
    misses: int = 0  # æœªå‘½ä¸­æ¬¡æ•°
    size: int = 0  # å½“å‰ç¼“å­˜å¤§å°

    @property
    def hit_rate(self) -> float:
        """è®¡ç®—ç¼“å­˜å‘½ä¸­ç‡"""
        total = self.hits + self.misses
        return self.hits / total if total > 0 else 0.0


class CacheBase(ABC):
    """ç¼“å­˜åŸºç±»ï¼Œå®šä¹‰ç¼“å­˜æ¥å£"""

    def __init__(self):
        self.stats = CacheStats()

    @abstractmethod
    def get(self, key: CacheKey) -> Optional[CacheValue]:
        """è·å–ç¼“å­˜å€¼"""
        pass

    @abstractmethod
    def set(self, key: CacheKey, value: CacheValue, ttl: Optional[int] = None) -> None:
        """è®¾ç½®ç¼“å­˜å€¼"""
        pass

    @abstractmethod
    def delete(self, key: CacheKey) -> None:
        """åˆ é™¤ç¼“å­˜å€¼"""
        pass

    @abstractmethod
    def clear(self) -> None:
        """æ¸…ç©ºç¼“å­˜"""
        pass

    def get_stats(self) -> CacheStats:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        return self.stats


class LRUCache(CacheBase):
    """LRUç¼“å­˜å®ç°"""

    def __init__(self, capacity: int = 128):
        """
        åˆå§‹åŒ–LRUç¼“å­˜

        Args:
            capacity: ç¼“å­˜æœ€å¤§å®¹é‡
        """
        super().__init__()
        self.capacity = capacity
        self.cache: OrderedDict = OrderedDict()
        self.ttl_map: Dict[CacheKey, float] = {}
        self.lock = Lock()

    def _check_ttl(self, key: CacheKey) -> bool:
        """æ£€æŸ¥é”®æ˜¯å¦è¿‡æœŸ"""
        if key in self.ttl_map:
            if time.time() > self.ttl_map[key]:
                self.delete(key)
                return False
        return True

    def get(self, key: CacheKey) -> Optional[CacheValue]:
        """
        è·å–ç¼“å­˜å€¼

        Args:
            key: ç¼“å­˜é”®

        Returns:
            Optional[CacheValue]: ç¼“å­˜å€¼ï¼Œä¸å­˜åœ¨åˆ™è¿”å›None
        """
        with self.lock:
            if key in self.cache and self._check_ttl(key):
                self.stats.hits += 1
                self.cache.move_to_end(key)
                my_logger.logger.debug(f"ğŸ¯ LRUç¼“å­˜å‘½ä¸­: {key}")
                return self.cache[key]
            self.stats.misses += 1
            my_logger.logger.debug(f"âŒ LRUç¼“å­˜æœªå‘½ä¸­: {key}")
            return None

    def set(self, key: CacheKey, value: CacheValue, ttl: Optional[int] = None) -> None:
        """
        è®¾ç½®ç¼“å­˜å€¼

        Args:
            key: ç¼“å­˜é”®
            value: ç¼“å­˜å€¼
            ttl: è¿‡æœŸæ—¶é—´(ç§’)
        """
        with self.lock:
            if key in self.cache:
                my_logger.logger.debug(f"ğŸ“ æ›´æ–°LRUç¼“å­˜: {key}")
                self.cache.move_to_end(key)
            else:
                if len(self.cache) >= self.capacity:
                    removed_key, _ = self.cache.popitem(last=False)
                    my_logger.logger.debug(
                        f"â™»ï¸ LRUç¼“å­˜å·²æ»¡ï¼Œç§»é™¤æœ€ä¹…æœªä½¿ç”¨é¡¹: {removed_key}")
                my_logger.logger.debug(f"ğŸ“ å†™å…¥LRUç¼“å­˜: {key}")
            self.cache[key] = value
            if ttl is not None:
                self.ttl_map[key] = time.time() + ttl
                my_logger.logger.debug(f"â±ï¸ è®¾ç½®è¿‡æœŸæ—¶é—´: {key}, TTL={ttl}ç§’")
            self.stats.size = len(self.cache)

    def delete(self, key: CacheKey) -> None:
        """
        åˆ é™¤ç¼“å­˜å€¼

        Args:
            key: ç¼“å­˜é”®
        """
        with self.lock:
            self.cache.pop(key, None)
            self.ttl_map.pop(key, None)
            self.stats.size = len(self.cache)
            my_logger.logger.debug(f"ğŸ—‘ï¸ åˆ é™¤LRUç¼“å­˜: {key}")

    def clear(self) -> None:
        """æ¸…ç©ºç¼“å­˜"""
        with self.lock:
            self.cache.clear()
            self.ttl_map.clear()
            self.stats.size = 0
            my_logger.logger.info("ğŸ§¹ æ¸…ç©ºLRUç¼“å­˜")


class RedisCache(CacheBase):
    """Redisç¼“å­˜å®ç°"""

    def __init__(self, host: str = 'localhost', port: int = 6379,
                 db: int = 0, password: Optional[str] = None,
                 prefix: str = 'cache:'):
        """
        åˆå§‹åŒ–Redisç¼“å­˜

        Args:
            host: Redisä¸»æœºåœ°å€
            port: Redisç«¯å£
            db: æ•°æ®åº“ç¼–å·
            password: å¯†ç 
            prefix: é”®å‰ç¼€
        """
        super().__init__()
        self.client = redis.Redis(
            host=host,
            port=port,
            db=db,
            password=password,
            decode_responses=True
        )
        self.prefix = prefix
        my_logger.logger.info(f"ğŸ“¦ åˆå§‹åŒ–Redisç¼“å­˜: {host}:{port}/{db}")

    def _make_key(self, key: CacheKey) -> str:
        """ç”Ÿæˆå¸¦å‰ç¼€çš„é”®"""
        return f"{self.prefix}{key}"

    def get(self, key: CacheKey) -> Optional[CacheValue]:
        """
        è·å–ç¼“å­˜å€¼

        Args:
            key: ç¼“å­˜é”®

        Returns:
            Optional[CacheValue]: ç¼“å­˜å€¼ï¼Œä¸å­˜åœ¨åˆ™è¿”å›None
        """
        full_key = self._make_key(key)
        value = self.client.get(full_key)
        if value is not None:
            self.stats.hits += 1
            my_logger.logger.debug(f"ğŸ¯ Redisç¼“å­˜å‘½ä¸­: {full_key}")
            try:
                if isinstance(value, (str, bytes, bytearray)):
                    return pickle.loads(value.encode('latin1') if isinstance(value, str) else value)
                else:
                    my_logger.logger.warning(f"âš ï¸ Redisè¿”å›äº†æ„å¤–çš„å€¼ç±»å‹: {type(value)}")
                    return None
            except (pickle.PickleError, ValueError, TypeError) as error:
                my_logger.logger.debug(f"ğŸ“„ Rediså€¼ä½¿ç”¨JSONè§£æ: {full_key}, Pickleè§£æå¤±è´¥: {error}")
                try:
                    if isinstance(value, (str, bytes, bytearray)):
                        return json.loads(value)
                    else:
                        my_logger.logger.warning(f"âš ï¸ Redisè¿”å›äº†æ„å¤–çš„å€¼ç±»å‹: {type(value)}")
                        return None
                except (json.JSONDecodeError, ValueError, TypeError) as error:
                    my_logger.logger.error(f"âŒ è§£æRediså€¼å¤±è´¥: {full_key}, é”™è¯¯: {error}")
                    return None
        self.stats.misses += 1
        my_logger.logger.debug(f"âŒ Redisç¼“å­˜æœªå‘½ä¸­: {full_key}")
        return None

    def set(self, key: CacheKey, value: CacheValue, ttl: Optional[int] = None) -> None:
        """
        è®¾ç½®ç¼“å­˜å€¼

        Args:
            key: ç¼“å­˜é”®
            value: ç¼“å­˜å€¼
            ttl: è¿‡æœŸæ—¶é—´(ç§’)
        """
        full_key = self._make_key(key)
        try:
            value_str = pickle.dumps(value)
            my_logger.logger.debug(f"ğŸ“ Redisä½¿ç”¨Pickleåºåˆ—åŒ–: {full_key}")
        except (pickle.PickleError, TypeError, AttributeError) as error:
            my_logger.logger.debug(f"âš ï¸ Pickleåºåˆ—åŒ–å¤±è´¥ï¼Œå°è¯•JSONåºåˆ—åŒ–: {full_key}, é”™è¯¯: {error}")
            try:
                value_str = json.dumps(value)
                my_logger.logger.debug(f"ğŸ“ Redisä½¿ç”¨JSONåºåˆ—åŒ–: {full_key}")
            except (TypeError, ValueError) as error:
                my_logger.logger.error(f"âŒ åºåˆ—åŒ–å¤±è´¥: {full_key}, é”™è¯¯: {error}")
                raise

        if ttl is not None:
            my_logger.logger.debug(f"â±ï¸ Redisè®¾ç½®è¿‡æœŸæ—¶é—´: {full_key}, TTL={ttl}ç§’")
            self.client.setex(full_key, ttl, value_str)
        else:
            my_logger.logger.debug(f"ğŸ“ å†™å…¥Redisç¼“å­˜: {full_key}")
            self.client.set(full_key, value_str)
        self.stats.size = self.client.dbsize()

    def delete(self, key: CacheKey) -> None:
        """
        åˆ é™¤ç¼“å­˜å€¼

        Args:
            key: ç¼“å­˜é”®
        """
        full_key = self._make_key(key)
        self.client.delete(full_key)
        self.stats.size = self.client.dbsize()
        my_logger.logger.debug(f"ğŸ—‘ï¸ åˆ é™¤Redisç¼“å­˜: {full_key}")

    def clear(self) -> None:
        """æ¸…ç©ºç¼“å­˜"""
        pattern = f"{self.prefix}*"
        keys = list(self.client.keys(pattern))
        if keys:
            self.client.delete(*keys)
        self.stats.size = 0
        my_logger.logger.info(f"ğŸ§¹ æ¸…ç©ºRedisç¼“å­˜: {pattern}")


class DiskCache(CacheBase):
    """ç£ç›˜ç¼“å­˜å®ç°"""

    _NAMESPACE = uuid.UUID("c875fb30-a8a8-402d-a796-225a6b065cad")

    def __init__(self, cache_path: Optional[str] = None):
        """
        åˆå§‹åŒ–ç£ç›˜ç¼“å­˜

        Args:
            cache_path: ç¼“å­˜ç›®å½•è·¯å¾„ï¼Œé»˜è®¤ä½¿ç”¨ç³»ç»Ÿä¸´æ—¶ç›®å½•
        """
        super().__init__()
        if cache_path:
            self.cache_path = os.path.abspath(cache_path)
        else:
            self.cache_path = DISK_CACHE_PATH

        if not os.path.exists(self.cache_path):
            os.makedirs(self.cache_path)
            my_logger.logger.info(f"ğŸ“ åˆ›å»ºç£ç›˜ç¼“å­˜ç›®å½•: {self.cache_path}")

    def _get_cache_file(self, key: CacheKey) -> str:
        """ç”Ÿæˆç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        key_str = str(key)
        params_uuid = uuid.uuid5(self._NAMESPACE, key_str)
        return os.path.join(self.cache_path, f"{key_str}-{params_uuid}.cache")

    def get(self, key: CacheKey) -> Optional[CacheValue]:
        """ä»ç£ç›˜è¯»å–ç¼“å­˜å€¼"""
        cache_file = self._get_cache_file(key)
        try:
            with open(cache_file, 'rb') as f:
                self.stats.hits += 1
                my_logger.logger.debug(f"ğŸ¯ ç£ç›˜ç¼“å­˜å‘½ä¸­: {cache_file}")
                return pickle.load(f)
        except (FileNotFoundError, pickle.PickleError) as e:
            self.stats.misses += 1
            my_logger.logger.debug(f"âŒ ç£ç›˜ç¼“å­˜æœªå‘½ä¸­: {cache_file}, åŸå› : {str(e)}")
            return None

    def set(self, key: CacheKey, value: CacheValue, ttl: Optional[int] = None) -> None:
        """å°†å€¼å†™å…¥ç£ç›˜ç¼“å­˜"""
        cache_file = self._get_cache_file(key)
        try:
            with open(cache_file, 'wb') as f:
                pickle.dump(value, cast(SupportsWrite[bytes], f))
                my_logger.logger.debug(f"ğŸ“ å†™å…¥ç£ç›˜ç¼“å­˜: {cache_file}")
            self.stats.size = len(os.listdir(self.cache_path))
        except (OSError, pickle.PickleError) as e:
            my_logger.logger.error(f"âŒ å†™å…¥ç£ç›˜ç¼“å­˜å¤±è´¥: {cache_file}, é”™è¯¯: {e}")

    def delete(self, key: CacheKey) -> None:
        """åˆ é™¤ç¼“å­˜æ–‡ä»¶"""
        cache_file = self._get_cache_file(key)
        try:
            os.remove(cache_file)
            self.stats.size = len(os.listdir(self.cache_path))
            my_logger.logger.debug(f"ğŸ—‘ï¸ åˆ é™¤ç£ç›˜ç¼“å­˜: {cache_file}")
        except FileNotFoundError:
            pass

    def clear(self) -> None:
        """æ¸…ç©ºç¼“å­˜ç›®å½•"""
        if os.path.exists(self.cache_path):
            shutil.rmtree(self.cache_path)
            os.makedirs(self.cache_path)
            my_logger.logger.info(f"ğŸ§¹ æ¸…ç©ºç£ç›˜ç¼“å­˜ç›®å½•: {self.cache_path}")
        self.stats.size = 0


class JsonDiskCache(CacheBase):
    """JSONæ–‡ä»¶ç¼“å­˜å®ç°"""

    def __init__(self, file_path: Optional[str] = None):
        """
        åˆå§‹åŒ–JSONæ–‡ä»¶ç¼“å­˜

        Args:
            file_path: JSONæ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä½¿ç”¨ç³»ç»Ÿä¸´æ—¶ç›®å½•
        """
        super().__init__()
        self.file_path = file_path or JSON_CACHE_PATH
        self.lock = Lock()

        # ç¡®ä¿ç¼“å­˜æ–‡ä»¶å­˜åœ¨
        if not os.path.exists(self.file_path):
            with open(self.file_path, "w", encoding="utf-8") as f:
                json.dump({}, cast(SupportsWrite[str], f))
            my_logger.logger.info(f"ğŸ“„ åˆ›å»ºJSONç¼“å­˜æ–‡ä»¶: {self.file_path}")

    def get(self, key: CacheKey) -> Optional[CacheValue]:
        """ä»JSONæ–‡ä»¶è¯»å–ç¼“å­˜å€¼"""
        with self.lock:
            try:
                with open(self.file_path, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    if str(key) in data:
                        self.stats.hits += 1
                        my_logger.logger.debug(f"ğŸ¯ JSONç¼“å­˜å‘½ä¸­: {key}")
                        return data[str(key)]
                    self.stats.misses += 1
                    my_logger.logger.debug(f"âŒ JSONç¼“å­˜æœªå‘½ä¸­: {key}")
                    return None
            except (json.JSONDecodeError, FileNotFoundError) as error:
                my_logger.logger.error(
                    f"âŒ è¯»å–JSONç¼“å­˜å¤±è´¥: {self.file_path}, é”™è¯¯: {error}")
                return None

    def set(self, key: CacheKey, value: CacheValue, ttl: Optional[int] = None) -> None:
        """å°†å€¼å†™å…¥JSONæ–‡ä»¶ç¼“å­˜"""
        with self.lock:
            try:
                with open(self.file_path, "r+", encoding="utf-8") as f:
                    data = json.load(f)
                    data[str(key)] = value
                    f.seek(0)
                    f.truncate()
                    json.dump(data, cast(SupportsWrite[str], f))
                    self.stats.size = len(data)
                    my_logger.logger.debug(f"ğŸ“ å†™å…¥JSONç¼“å­˜: {key}")
            except (json.JSONDecodeError, FileNotFoundError) as error:
                my_logger.logger.warning(f"âš ï¸ JSONç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨æˆ–æŸåï¼Œåˆ›å»ºæ–°æ–‡ä»¶: {error}")
                with open(self.file_path, "w", encoding="utf-8") as f:
                    json.dump({str(key): value}, cast(SupportsWrite[str], f))
                    self.stats.size = 1

    def delete(self, key: CacheKey) -> None:
        """ä»JSONæ–‡ä»¶åˆ é™¤ç¼“å­˜å€¼"""
        with self.lock:
            try:
                with open(self.file_path, "r+", encoding="utf-8") as f:
                    data = json.load(f)
                    if str(key) in data:
                        del data[str(key)]
                        f.seek(0)
                        f.truncate()
                        json.dump(data, cast(SupportsWrite[str], f))
                        self.stats.size = len(data)
                        my_logger.logger.debug(f"ğŸ—‘ï¸ åˆ é™¤JSONç¼“å­˜: {key}")
            except (json.JSONDecodeError, FileNotFoundError):
                pass

    def clear(self) -> None:
        """æ¸…ç©ºJSONæ–‡ä»¶ç¼“å­˜"""
        with self.lock:
            with open(self.file_path, "w", encoding="utf-8") as f:
                json.dump({}, cast(SupportsWrite[str], f))
            self.stats.size = 0
            my_logger.logger.info(f"ğŸ§¹ æ¸…ç©ºJSONç¼“å­˜æ–‡ä»¶: {self.file_path}")


class CacheFactory:
    """ç¼“å­˜å·¥å‚ç±»"""

    @staticmethod
    def create(cache_type: str = 'lru', **kwargs: Any) -> CacheBase:
        """
        åˆ›å»ºç¼“å­˜å®ä¾‹

        Args:
            cache_type: ç¼“å­˜ç±»å‹ ('lru'ã€'redis'ã€'disk'ã€'json')
            **kwargs: ç¼“å­˜é…ç½®å‚æ•°

        Returns:
            CacheBase: ç¼“å­˜å®ä¾‹
        """
        if cache_type == 'lru':
            return LRUCache(**kwargs)
        elif cache_type == 'redis':
            return RedisCache(**kwargs)
        elif cache_type == 'disk':
            return DiskCache(**kwargs)
        elif cache_type == 'json':
            return JsonDiskCache(**kwargs)
        else:
            raise ValueError(f"Unsupported cache type: {cache_type}")


def cache(cache_instance: Optional[CacheBase] = None,
          ttl: Optional[int] = None,
          key_prefix: str = "",
          key_generator: Optional[Callable[..., str]] = None) -> Callable:
    """
    ç¼“å­˜è£…é¥°å™¨

    Args:
        cache_instance: ç¼“å­˜å®ä¾‹ï¼Œé»˜è®¤ä½¿ç”¨LRUç¼“å­˜
        ttl: ç¼“å­˜è¿‡æœŸæ—¶é—´(ç§’)
        key_prefix: ç¼“å­˜é”®å‰ç¼€
        key_generator: è‡ªå®šä¹‰ç¼“å­˜é”®ç”Ÿæˆå‡½æ•°

    Returns:
        Callable: è£…é¥°å™¨å‡½æ•°

    Example:
        >>> # ä½¿ç”¨é»˜è®¤LRUç¼“å­˜
        >>> @cache(ttl=300)
        ... def get_user(user_id: int) -> dict:
        ...     return {"id": user_id, "name": "test"}

        >>> # ä½¿ç”¨Redisç¼“å­˜
        >>> redis_cache = RedisCache(host='localhost', port=6379)
        >>> @cache(cache_instance=redis_cache, ttl=300)
        ... def get_user(user_id: int) -> dict:
        ...     return {"id": user_id, "name": "test"}
    """
    if cache_instance is None:
        cache_instance = LRUCache()

    def decorator(func: Callable[..., T]) -> Callable[..., T]:
        @functools.wraps(func)
        def wrapper(*args: Any, **kwargs: Any) -> T:
            # ç”Ÿæˆç¼“å­˜é”®
            if key_generator is not None:
                cache_key = key_generator(*args, **kwargs)
            else:
                # é»˜è®¤ä½¿ç”¨å‡½æ•°åå’Œå‚æ•°ç”Ÿæˆé”®
                params = [str(arg) for arg in args]
                params.extend(f"{k}={v}" for k, v in sorted(kwargs.items()))
                cache_key = f"{key_prefix}{func.__name__}:{':'.join(params)}"

            # å°è¯•ä»ç¼“å­˜è·å–
            cached_value = cache_instance.get(cache_key)
            if cached_value is not None:
                my_logger.logger.debug(f"ğŸ¯ ç¼“å­˜å‘½ä¸­: {cache_key}")
                return cast(T, cached_value)

            # æ‰§è¡Œå‡½æ•°å¹¶ç¼“å­˜ç»“æœ
            my_logger.logger.debug(f"âŒ ç¼“å­˜æœªå‘½ä¸­: {cache_key}")
            result = func(*args, **kwargs)
            cache_instance.set(cache_key, result, ttl)
            my_logger.logger.debug(f"ğŸ’¾ å·²ç¼“å­˜ç»“æœ: {cache_key}")
            return result

        # æ·»åŠ ç¼“å­˜ç®¡ç†æ–¹æ³•
        wrapper.clear_cache = cache_instance.clear  # type: ignore
        wrapper.get_stats = cache_instance.get_stats  # type: ignore

        return wrapper

    return decorator


def dependent_cache(func_obj: Callable, key_name: Text = None, cache_instance: Optional[CacheBase] = None,
                    ttl: Optional[int] = None, *out_args, **out_kwargs) -> Callable:
    """
    ä¾èµ–å‡½æ•°ç¼“å­˜è£…é¥°å™¨

    Args:
        func_obj: ä¾èµ–çš„å‡½æ•°å¯¹è±¡
        key_name: ç¼“å­˜é”®åï¼Œé»˜è®¤ä½¿ç”¨ä¾èµ–å‡½æ•°å
        cache_instance: ç¼“å­˜å®ä¾‹ï¼Œé»˜è®¤ä½¿ç”¨LRUç¼“å­˜
        ttl: ç¼“å­˜è¿‡æœŸæ—¶é—´(ç§’)
        out_args: ä¾èµ–å‡½æ•°çš„ä½ç½®å‚æ•°
        out_kwargs: ä¾èµ–å‡½æ•°çš„å…³é”®å­—å‚æ•°

    Returns:
        Callable: è£…é¥°å™¨å‡½æ•°

    Example:
        >>> def get_token():
        ...     return "token123"
        ...
        >>> @dependent_cache(get_token, ttl=300)
        ... def api_request():
        ...     pass
    """
    if cache_instance is None:
        cache_instance = LRUCache()

    def decorator(func: Callable[..., T]) -> Callable[..., T]:
        @functools.wraps(func)
        def wrapper(*args: Any, **kwargs: Any) -> T:
            func_name = func.__name__
            depend_func_name = func_obj.__name__

            # ä½¿ç”¨æŒ‡å®šçš„é”®åæˆ–å‡½æ•°åä½œä¸ºç¼“å­˜é”®
            cache_key = key_name or depend_func_name

            # æ£€æŸ¥ç¼“å­˜ä¸­æ˜¯å¦å­˜åœ¨ä¾èµ–å‡½æ•°çš„ç»“æœ
            cached_value = cache_instance.get(cache_key)
            if cached_value is None:
                my_logger.logger.info(
                    f"ğŸ”— <{func_name}> ä¾èµ– <{depend_func_name}>, æ‰§è¡Œä¾èµ–å‡½æ•°")
                # æ‰§è¡Œä¾èµ–å‡½æ•°å¹¶ç¼“å­˜ç»“æœ
                result = func_obj(*out_args, **out_kwargs)
                cache_instance.set(cache_key, result, ttl)
            else:
                my_logger.logger.info(
                    f"ğŸ”— <{depend_func_name}> å·²æ‰§è¡Œ, ä»ç¼“å­˜è·å–ç»“æœ: {cache_key}")

            # æ‰§è¡Œä¸»å‡½æ•°
            return func(*args, **kwargs)

        # æ·»åŠ ç¼“å­˜ç®¡ç†æ–¹æ³•
        wrapper.clear_cache = cache_instance.clear  # type: ignore
        wrapper.get_stats = cache_instance.get_stats  # type: ignore

        return wrapper

    return decorator


# åˆ›å»ºé»˜è®¤ç¼“å­˜å®ä¾‹
default_cache = LRUCache()
default_disk_cache = DiskCache()
default_json_cache = JsonDiskCache()
